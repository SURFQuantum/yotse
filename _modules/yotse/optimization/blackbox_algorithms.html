<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>yotse.optimization.blackbox_algorithms &mdash; YOTSE  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="https://unpkg.com/mermaid@10.2.0/dist/mermaid.min.js"></script>
        <script>mermaid.initialize({startOnLoad:true});</script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            YOTSE
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../yotse.html">Yotse Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples.html">Yotse Examples</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">YOTSE</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">yotse.optimization.blackbox_algorithms</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for yotse.optimization.blackbox_algorithms</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Collection of Subclasses of :class:GenericOptimization implementing different</span>
<span class="sd">optimization algorithms.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">bayes_opt</span> <span class="kn">import</span> <span class="n">BayesianOptimization</span>
<span class="kn">from</span> <span class="nn">pygad.pygad</span> <span class="kn">import</span> <span class="n">GA</span>

<span class="kn">from</span> <span class="nn">yotse.optimization.generic_optimization</span> <span class="kn">import</span> <span class="n">GenericOptimization</span>
<span class="kn">from</span> <span class="nn">yotse.optimization.modded_pygad_ga</span> <span class="kn">import</span> <span class="n">ModGA</span>  <span class="c1"># type: ignore[attr-defined]</span>
<span class="kn">from</span> <span class="nn">yotse.pre</span> <span class="kn">import</span> <span class="n">ConstraintDict</span>
<span class="kn">from</span> <span class="nn">yotse.utils.utils</span> <span class="kn">import</span> <span class="n">ndarray_to_list</span>


<div class="viewcode-block" id="GAOpt">
<a class="viewcode-back" href="../../../yotse.optimization.html#yotse.optimization.blackbox_algorithms.GAOpt">[docs]</a>
<span class="k">class</span> <span class="nc">GAOpt</span><span class="p">(</span><span class="n">GenericOptimization</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Genetic algorithm.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    blackbox_optimization: bool</span>
<span class="sd">        Whether this is used as a blackbox optimization.</span>
<span class="sd">    initial_data_points: np.ndarray</span>
<span class="sd">        Initial population of data points to start the optimization with.</span>
<span class="sd">    num_generations : int</span>
<span class="sd">        Number of generations in the genetic algorithm.</span>
<span class="sd">    num_parents_mating : int</span>
<span class="sd">        Number of solutions to be selected as parents in the genetic algorithm.</span>
<span class="sd">    fitness_func : function (optional)</span>
<span class="sd">        Fitness/objective/cost function/function to optimize. Only needed if `blackbox_optimization=False`.</span>
<span class="sd">        Default is None.</span>
<span class="sd">    gene_space : dict or list (optional)</span>
<span class="sd">        Dictionary with constraints. Keys can be &#39;low&#39;, &#39;high&#39; and &#39;step&#39;. Alternatively list with acceptable values or</span>
<span class="sd">        list of dicts. If only single object is passed it will be applied for all input parameters, otherwise a</span>
<span class="sd">        separate list or dict has to be supplied for each parameter.</span>
<span class="sd">        Defaults to None.</span>
<span class="sd">    refinement_factors : list (optional)</span>
<span class="sd">        Refinement factors for each active parameter in the optimization in range [0.,1.] to be used for manual</span>
<span class="sd">        grid point generation.</span>
<span class="sd">        Defaults to None.</span>
<span class="sd">    logging_level : int (optional)</span>
<span class="sd">        Level of logging: 1 - only essential data; 2 - include plots; 3 - dump everything.</span>
<span class="sd">        Defaults to 1.</span>
<span class="sd">    allow_duplicate_genes : bool (optional)</span>
<span class="sd">        If True, then a solution/chromosome may have duplicate gene values.</span>
<span class="sd">        If False, then each gene will have a unique value in its solution.</span>
<span class="sd">        Defaults to False.</span>
<span class="sd">    pygad_kwargs : (optional)</span>
<span class="sd">        Optional pygad arguments to be passed to `pygad.GA`.</span>
<span class="sd">        See https://pygad.readthedocs.io/en/latest/README_pygad_ReadTheDocs.html#pygad-ga-class for documentation.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    constraints : dict or list</span>
<span class="sd">        Constraints to check for during generation of new points.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="GAOpt.__init__">
<a class="viewcode-back" href="../../../yotse.optimization.html#yotse.optimization.blackbox_algorithms.GAOpt.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">blackbox_optimization</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">initial_data_points</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">num_generations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_parents_mating</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">gene_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ConstraintDict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">refinement_factors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">allow_duplicate_genes</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">fitness_func</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">pygad_kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the GAOpt object.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">blackbox_optimization</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">fitness_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;blackbox_optimization set to True, but fitness_func is not None.&quot;</span>
                <span class="p">)</span>
            <span class="n">fitness_func</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_params_to_cost_value</span>

        <span class="c1"># Note: number of new points is determined by initial population</span>
        <span class="c1"># gene_space to limit space in which new genes are formed = constraints</span>
        <span class="n">ga_instance</span> <span class="o">=</span> <span class="n">ModGA</span><span class="p">(</span>
            <span class="n">fitness_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_objective_func</span><span class="p">,</span>
            <span class="n">initial_population</span><span class="o">=</span><span class="n">ndarray_to_list</span><span class="p">(</span><span class="n">initial_data_points</span><span class="p">),</span>
            <span class="n">num_generations</span><span class="o">=</span><span class="n">num_generations</span><span class="p">,</span>
            <span class="n">num_parents_mating</span><span class="o">=</span><span class="n">num_parents_mating</span><span class="p">,</span>
            <span class="c1"># todo : gene_type/_space are exactly data_type/constraints of the params, see core.py</span>
            <span class="c1"># gene_type=gene_type,</span>
            <span class="n">gene_space</span><span class="o">=</span><span class="n">gene_space</span><span class="p">,</span>
            <span class="n">save_best_solutions</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">allow_duplicate_genes</span><span class="o">=</span><span class="n">allow_duplicate_genes</span><span class="p">,</span>
            <span class="n">mutation_by_replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="o">**</span><span class="n">pygad_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blackbox_optimization</span> <span class="o">=</span> <span class="n">blackbox_optimization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">constraints</span> <span class="o">=</span> <span class="n">gene_space</span>
        <span class="c1"># todo : why if save_solutions=True the optimization doesn&#39;t converge anymore?</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">function</span><span class="o">=</span><span class="n">fitness_func</span><span class="p">,</span>  <span class="c1"># type: ignore [arg-type]</span>
            <span class="n">opt_instance</span><span class="o">=</span><span class="n">ga_instance</span><span class="p">,</span>
            <span class="n">refinement_factors</span><span class="o">=</span><span class="n">refinement_factors</span><span class="p">,</span>
            <span class="n">logging_level</span><span class="o">=</span><span class="n">logging_level</span><span class="p">,</span>
            <span class="n">extrema</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">MINIMUM</span><span class="p">,</span>
            <span class="n">evolutionary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span></div>


    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">current_datapoints</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the current datapoints that will be used if an optimization is started</span>
<span class="sd">        now.</span>

<span class="sd">        In this case it is the population.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_instance</span><span class="o">.</span><span class="n">population</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">max_iterations</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return maximum number of iterations of GA = number of generation.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimization_instance</span><span class="o">.</span><span class="n">num_generations</span><span class="p">)</span>

<div class="viewcode-block" id="GAOpt._objective_func">
<a class="viewcode-back" href="../../../yotse.optimization.html#yotse.optimization.blackbox_algorithms.GAOpt._objective_func">[docs]</a>
    <span class="k">def</span> <span class="nf">_objective_func</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">ga_instance</span><span class="p">:</span> <span class="n">GA</span><span class="p">,</span> <span class="n">solution</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">solution_idx</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fitness function to be called from PyGAD.</span>

<span class="sd">        Wrapper around the actual function to give pygad some more functionality.</span>
<span class="sd">        First, it adds the possibility to choose whether to max-/minimize the fitness.</span>
<span class="sd">        Second, it removes the necessity to pass the ga_instance to the function, thus making the implementation</span>
<span class="sd">        more general.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ga_instance</span>
<span class="sd">            Instance of pygad.GA.</span>
<span class="sd">        solution : List[float]</span>
<span class="sd">            List of solutions.</span>
<span class="sd">        solution_idx : int</span>
<span class="sd">            Index of solution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Fitness value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Invert function to find the minimum, if needed</span>
        <span class="n">factor</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">extrema</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">MINIMUM</span><span class="p">:</span>
            <span class="n">factor</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">blackbox_optimization</span><span class="p">:</span>
            <span class="c1"># passing params to self.input_params_to_cost_value</span>
            <span class="n">fitness</span> <span class="o">=</span> <span class="n">factor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">solution</span><span class="p">,</span> <span class="n">solution_idx</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># passing params to the function of type Callable[...,float]</span>
            <span class="n">fitness</span> <span class="o">=</span> <span class="n">factor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">solution</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">logging_level</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">solution</span><span class="p">,</span> <span class="n">solution_idx</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">fitness</span></div>


<div class="viewcode-block" id="GAOpt.execute">
<a class="viewcode-back" href="../../../yotse.optimization.html#yotse.optimization.blackbox_algorithms.GAOpt.execute">[docs]</a>
    <span class="k">def</span> <span class="nf">execute</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Execute single step in the genetic algorithm.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimization_instance</span><span class="o">.</span><span class="n">run_single_generation</span><span class="p">()</span>

        <span class="c1"># Report convergence</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">logging_level</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimization_instance</span><span class="o">.</span><span class="n">plot_fitness</span><span class="p">()</span></div>


<div class="viewcode-block" id="GAOpt.get_best_solution">
<a class="viewcode-back" href="../../../yotse.optimization.html#yotse.optimization.blackbox_algorithms.GAOpt.get_best_solution">[docs]</a>
    <span class="k">def</span> <span class="nf">get_best_solution</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>  <span class="c1"># type: ignore[override]</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the best solution. We don&#39;t yet know the fitness for the solution</span>
<span class="sd">        (because we have not run the simulation for those values yet), so just return</span>
<span class="sd">        the point.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        solution, solution_fitness, solution_idx</span>
<span class="sd">            Solution its fitness and its index in the list of cost function solutions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_instance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Trying to `get_best_solution`, but GA instance not initialized.&quot;</span>
            <span class="p">)</span>
        <span class="n">best_solution</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_instance</span><span class="o">.</span><span class="n">best_solutions</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># solution_idx = self.ga_instance.population.tolist().index(best_solution)</span>
        <span class="k">return</span> <span class="n">best_solution</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span></div>

        <span class="c1"># todo: this could also instead return solution and fitness of the best solution one generation back?</span>

<div class="viewcode-block" id="GAOpt.get_new_points">
<a class="viewcode-back" href="../../../yotse.optimization.html#yotse.optimization.blackbox_algorithms.GAOpt.get_new_points">[docs]</a>
    <span class="k">def</span> <span class="nf">get_new_points</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get new points from the GA (aka return the next population).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        new_points : np.ndarray</span>
<span class="sd">            New points for the next iteration of the optimization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">new_points</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_instance</span><span class="o">.</span><span class="n">population</span>
        <span class="c1"># todo: see if we check constraints somewhere else, might be redundant</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">constraints</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># double check constraints are kept</span>
            <span class="k">for</span> <span class="n">point</span> <span class="ow">in</span> <span class="n">new_points</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">point</span><span class="p">):</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="k">assert</span> <span class="p">(</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">[</span><span class="n">index</span><span class="p">][</span><span class="s2">&quot;low&quot;</span><span class="p">]</span>
                                <span class="o">&lt;=</span> <span class="n">value</span>
                                <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">[</span><span class="n">index</span><span class="p">][</span><span class="s2">&quot;high&quot;</span><span class="p">]</span>
                            <span class="p">)</span>
                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                        <span class="k">assert</span> <span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">[</span><span class="s2">&quot;low&quot;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">value</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">[</span><span class="s2">&quot;high&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unacceptable type </span><span class="si">{</span><span class="nb">type</span><span class="si">}</span><span class="s2"> for constraints.&quot;</span><span class="p">)</span>
        <span class="n">new_points</span> <span class="o">=</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">point</span><span class="p">)</span> <span class="k">for</span> <span class="n">point</span> <span class="ow">in</span> <span class="n">new_points</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">new_points</span><span class="p">)</span></div>


<div class="viewcode-block" id="GAOpt.overwrite_internal_data_points">
<a class="viewcode-back" href="../../../yotse.optimization.html#yotse.optimization.blackbox_algorithms.GAOpt.overwrite_internal_data_points">[docs]</a>
    <span class="k">def</span> <span class="nf">overwrite_internal_data_points</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_points</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Overwrite internal GA population with new datapoints from experiment.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimization_instance</span><span class="o">.</span><span class="n">population</span> <span class="o">=</span> <span class="n">data_points</span></div>
</div>



<div class="viewcode-block" id="BayesOpt">
<a class="viewcode-back" href="../../../yotse.optimization.html#yotse.optimization.blackbox_algorithms.BayesOpt">[docs]</a>
<span class="k">class</span> <span class="nc">BayesOpt</span><span class="p">(</span><span class="n">GenericOptimization</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Bayesian optimization.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    blackbox_optimization: bool</span>
<span class="sd">        Whether this is used as a blackbox optimization.</span>
<span class="sd">    pbounds: dict</span>
<span class="sd">        Dictionary with parameters names as keys and a tuple with minimum</span>
<span class="sd">        and maximum values.</span>
<span class="sd">    initial_data_points: np.ndarray (optional)</span>
<span class="sd">        Initial population of data points to start the optimization with.</span>
<span class="sd">        If none are specified lets the algorithm suggest a point. Defaults to None.</span>
<span class="sd">    fitness_func : function (optional)</span>
<span class="sd">        Fitness/objective/cost function/function to optimize. Only needed if `blackbox_optimization=False`.</span>
<span class="sd">        Default is None.</span>
<span class="sd">    logging_level : int (optional)</span>
<span class="sd">        Level of logging: 1 - only essential data; 2 - include plots; 3 - dump everything.</span>
<span class="sd">        Defaults to 1.</span>
<span class="sd">    bayesopt_kwargs : (optional)</span>
<span class="sd">        Optional arguments to be passed to `bayes_opt.BayesianOptimization`.</span>
<span class="sd">        See the documentation of that class for more info.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="BayesOpt.__init__">
<a class="viewcode-back" href="../../../yotse.optimization.html#yotse.optimization.blackbox_algorithms.BayesOpt.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">blackbox_optimization</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">pbounds</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
        <span class="n">initial_data_points</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">naive_parallelization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">grid_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">refinement_factors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fitness_func</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="o">**</span><span class="n">bayesopt_kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Bayesian optimization.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">blackbox_optimization</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">fitness_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;blackbox_optimization set to True, but fitness_func is not None.&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Whitebox optimization with BayesOpt not implemented...yet.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;utility_function&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">bayesopt_kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;utility_function must be specified for Bayesian Optimization.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">utility_function</span> <span class="o">=</span> <span class="n">bayesopt_kwargs</span><span class="p">[</span><span class="s2">&quot;utility_function&quot;</span><span class="p">]</span>
        <span class="n">bayesopt_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;utility_function&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;n_iter&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">bayesopt_kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;n_iter must be specified for Bayesian Optimization.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_iterations</span> <span class="o">=</span> <span class="n">bayesopt_kwargs</span><span class="p">[</span><span class="s2">&quot;n_iter&quot;</span><span class="p">]</span>
        <span class="n">bayesopt_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;n_iter&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">naive_parallelization</span> <span class="o">=</span> <span class="n">naive_parallelization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span> <span class="o">=</span> <span class="n">grid_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pbounds</span> <span class="o">=</span> <span class="n">pbounds</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">BayesianOptimization</span><span class="p">(</span>
            <span class="n">f</span><span class="o">=</span><span class="n">fitness_func</span><span class="p">,</span>
            <span class="n">pbounds</span><span class="o">=</span><span class="n">pbounds</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="o">**</span><span class="n">bayesopt_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># warn about discrete values</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="s2">&quot;WARNING: Bayesian Optimization does not currently implement discrete variables. See &quot;</span>
            <span class="s2">&quot;https://github.com/bayesian-optimization/BayesianOptimization/blob/master/examples/advanced-tour.ipynb&quot;</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">utility_function</span><span class="p">,</span>
            <span class="n">refinement_factors</span><span class="o">=</span><span class="n">refinement_factors</span><span class="p">,</span>
            <span class="n">opt_instance</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
            <span class="n">logging_level</span><span class="o">=</span><span class="n">logging_level</span><span class="p">,</span>
            <span class="n">extrema</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">MINIMUM</span><span class="p">,</span>
            <span class="n">evolutionary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># set initial point to investigate</span>
        <span class="k">if</span> <span class="n">initial_data_points</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">next_point_to_probe</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">suggest</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">utility_function</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">naive_parallelization</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">overwrite_internal_data_points</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">create_points_around_suggestion</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_point_to_probe</span><span class="p">)</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">overwrite_internal_data_points</span><span class="p">(</span><span class="n">initial_data_points</span><span class="p">)</span></div>


    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">current_datapoints</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the current datapoints that will be used if an optimization is started</span>
<span class="sd">        now.</span>

<span class="sd">        In this case it is the currently suggested point.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_point_to_probe</span>
        <span class="c1"># handle single or multiple points</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">key</span><span class="p">:</span> <span class="p">[</span><span class="n">value</span><span class="p">]</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="n">value</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">input_data</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>
        <span class="n">next_point</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
        <span class="c1"># transpose to get desired shape (num_data_points, num_params)</span>
        <span class="n">next_point</span> <span class="o">=</span> <span class="n">next_point</span><span class="o">.</span><span class="n">T</span>

        <span class="k">return</span> <span class="n">next_point</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">max_iterations</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return maximum number of iterations of bayesian optimization.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_iterations</span><span class="p">)</span>

<div class="viewcode-block" id="BayesOpt.execute">
<a class="viewcode-back" href="../../../yotse.optimization.html#yotse.optimization.blackbox_algorithms.BayesOpt.execute">[docs]</a>
    <span class="k">def</span> <span class="nf">execute</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Execute single step in the bayesian optimization.&quot;&quot;&quot;</span>
        <span class="c1"># Note this should be run after the user script has been executed with input next_point_to_probe</span>
        <span class="n">target_column</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_param_cost_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_param_cost_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
            <span class="n">target_point</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="n">target_column</span><span class="p">]</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">extrema</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">MINIMUM</span><span class="p">:</span>
                <span class="c1"># minimize = find max of negative cost</span>
                <span class="n">target_point</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>

            <span class="n">params</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">target_column</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimization_instance</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target_point</span><span class="p">)</span></div>


<div class="viewcode-block" id="BayesOpt.get_best_solution">
<a class="viewcode-back" href="../../../yotse.optimization.html#yotse.optimization.blackbox_algorithms.BayesOpt.get_best_solution">[docs]</a>
    <span class="k">def</span> <span class="nf">get_best_solution</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the best solution. Should be implemented in every derived class.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        solution, solution_fitness, solution_idx</span>
<span class="sd">            Solution its fitness and its index in the list of data points.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># todo: this does not output solution, solution_fitness, solution_idx yet but params, cost, 0</span>
        <span class="c1"># todo: question is what solution_idx would even mean here</span>
        <span class="n">solution</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_instance</span><span class="o">.</span><span class="n">max</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">extrema</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">MINIMUM</span><span class="p">:</span>
            <span class="c1"># maximized neg cost, converting to pos cost again</span>
            <span class="n">solution</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimization_instance</span><span class="o">.</span><span class="n">max</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span>
            <span class="n">solution</span><span class="p">,</span>
            <span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="BayesOpt.get_new_points">
<a class="viewcode-back" href="../../../yotse.optimization.html#yotse.optimization.blackbox_algorithms.BayesOpt.get_new_points">[docs]</a>
    <span class="k">def</span> <span class="nf">get_new_points</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get new points from the BayesianOptimization instance. This is done via the</span>
<span class="sd">        `suggest` function.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        new_points : np.ndarray</span>
<span class="sd">            New points for the next iteration of the optimization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">next_point</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_instance</span><span class="o">.</span><span class="n">suggest</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">utility_function</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">naive_parallelization</span><span class="p">:</span>
            <span class="n">new_points</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_points_around_suggestion</span><span class="p">(</span><span class="n">next_point</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">list</span><span class="p">(</span><span class="n">next_point</span><span class="o">.</span><span class="n">values</span><span class="p">())])</span>
        <span class="k">return</span> <span class="n">new_points</span></div>


<div class="viewcode-block" id="BayesOpt.create_points_around_suggestion">
<a class="viewcode-back" href="../../../yotse.optimization.html#yotse.optimization.blackbox_algorithms.BayesOpt.create_points_around_suggestion">[docs]</a>
    <span class="k">def</span> <span class="nf">create_points_around_suggestion</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">suggested_point</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;BayesOpt only suggest a single point per iteration. In order to parallelize</span>
<span class="sd">        the optimization we use this function to create multiple data points to evaluate</span>
<span class="sd">        per iteration.</span>

<span class="sd">        Note: This current implementation is by now way optimal! This is the most naive and simple way to create</span>
<span class="sd">        multiple points to evaluate and should be improved in the future.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">param_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">suggested_point</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">param_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">suggested_point</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="n">param_bounds</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pbounds</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="n">grid_points</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_names</span><span class="p">):</span>
            <span class="c1"># calculate new ranges for each param</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">refinement_factors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;refinement_factors can not be None when creating points based on them.&quot;</span>
                <span class="p">)</span>
            <span class="n">delta_param</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">refinement_factors</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
                <span class="o">*</span> <span class="p">(</span><span class="n">param_bounds</span><span class="p">[</span><span class="n">p</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">param_bounds</span><span class="p">[</span><span class="n">p</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
                <span class="o">*</span> <span class="mf">0.5</span>
            <span class="p">)</span>
            <span class="n">grid_range</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">param_values</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">-</span> <span class="n">delta_param</span><span class="p">,</span>
                <span class="n">param_values</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta_param</span><span class="p">,</span>
            <span class="p">]</span>
            <span class="n">new_points_in_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
                <span class="n">grid_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">grid_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span>
            <span class="p">)</span>

            <span class="n">grid_points</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_points_in_grid</span><span class="p">)</span>

        <span class="c1"># Create a meshgrid from the grid points</span>
        <span class="n">grid_mesh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="o">*</span><span class="n">grid_points</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">grid_mesh</span></div>


<div class="viewcode-block" id="BayesOpt.overwrite_internal_data_points">
<a class="viewcode-back" href="../../../yotse.optimization.html#yotse.optimization.blackbox_algorithms.BayesOpt.overwrite_internal_data_points">[docs]</a>
    <span class="k">def</span> <span class="nf">overwrite_internal_data_points</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_points</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;After we generated a new point with get_new_points this function can be used</span>
<span class="sd">        to write that data point (or another point to investigate next) to the class.&quot;&quot;&quot;</span>
        <span class="n">param_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_instance</span><span class="o">.</span><span class="n">_space</span><span class="o">.</span><span class="n">_keys</span>

        <span class="n">new_point_from_array</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_keys</span><span class="p">):</span>
            <span class="n">new_point_from_array</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_points</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">next_point_to_probe</span> <span class="o">=</span> <span class="n">new_point_from_array</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, SURFQuantum.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>